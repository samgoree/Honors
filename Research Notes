Research Notes:

I plan to take notes pretty often on what I'm doing with honors.

Sunday, 9/11/16

The second week of the semester is over now. I didn't do anything the first week, and the second week I made a schedule and started working on midi_parser.py
I'm now realizing that my dataset from UMontreal does not have note on/off for the same pitch if the voice switches. This makes sense, since it reduces the size of the midi file without impacting the sound at all, but it's bad for me because I need to split those notes up somehow. My algorithm with voice crossing and voice overlap not allowed works, though, so as soon as I solve this problem, I'll be done cleaning the data I've got.

Tuesday 9/13/16

More bug fixes to midi_parser.py. Hopefully, I can finish this up by tomorrow/thursday and start on real ML

Wednesday 9/14/16

Works! A few edits to the way I was setting upper/lower bounds made it guess correctly on examples most of the time, when it doesn't know, though, I should make it choose closest voice not lowest

Thursday 9/15/16

I messed up huge. While opening parse_midi.py, I saw there was a swap file and overwrote whatever was in the swap file without really thinking. That was an early version of the program that didn't do anything, I lost all of my work. I got some of it back using the parse_midi.py~ file that vim creates as a backup, but that was still from Tuesday. I lost a few hours of work. I'm going to try to get all of that done again today.

I managed to fix all of the lost stuff, but now I'm running into a problematic passage that no amount of algorithm can fix. I've got a crazy idea: when the rules don't specify, randomly choose a voice and continue - that way it will probably get the right answer at some point.

Friday 9/16/16

I did a bit of reorganization today and also explored additional training sets, I just want to account for everything while writing code for models. I'm currently looking at Music21, a library for music information retrival from MIT that has a bunch of bach and palestrina and other stuff that I might find useful.

Saturday 9/17/16

I started working on the generative model today, and quickly realized I was unsure what I wanted to accomplish. The easy answer is to say that I want to try a bunch of things, but I'm not sure. For now, I'm going with the idea that given the previous timestep and the current timestep in some number of voices, I want to predict the next time step of another voice.

Wednesday 9/21/16

I kept working on the generative model a bit, and read a cool article by Anna Jordanous on evaluating computational creativity research.

Thursday 9/29/16

Sorry for few notes the past few days, I didn't do much work, mostly debugging the generative model (I spent way too long on a bug that ended up being order of operations (a + b * c != (a+b) * c). I also got word back from the two people I had emailed about grad school stuff, they both basically told me to keep shopping around the way I had (and also talk to Bob Keller, which I did). I'm going to let the grad school stuff rest for a week while I get back on track with honors work.

Other than that, the GRE went well and I'm on my way. If I want to sample from a custom distribution over discrete categories, how do I do that? Never mind, it was in some old code I had written, rng.choice lets you pass in a distribution. Now I'm trying to figure out a mismatched type error

Nope, I called the wrong function.

Got the model working! It produces output! I'm going to work on a way to output midi instead of midi numbers.

And I did that, what a productive evening! I'm going home.

Friday 9/30/16

It did the thing! I have results from the simple generative model. They're not very good.

TODO:
Mini-batch training seems like a must
Validation to prevent over-fitting
Refactor to allow for modular note encodings
Product-of-experts (rhythm, pitch, contour)

Future:
Autoencoders?
Convolution across time? (a la WaveNet)

If I look at the loss function over time, it's not training really at all. I think I messed something up, but I don't know what.

Wednesday 10/5/16

Time to refactor and make sure everything is ok

minibatches - each piece needs to be the same length
my plan: randomly choose pieces, pick 8-timestep segments from each piece chosen at random

This stuff is done now, but everything's buggy.

I need to learn how convolution actually works - what would a one-dimensional convolution with kernel of size 1xk mean? Maybe I should ask Adam

Anyway, I also read the pixel-rnn article. The math is a bit confusing, but I get the gist of what they are doing.

Thursday 10/6/16

Now I'm getting to the meat of the problem: I'm not storing 2 dimensions of internal state

Should a section actually snag the timestep before as well, so it can feed that in rather than zeros? How can I do that while still allowing the start of a piece to be used?
Figured it out, doing all of that as processing outside of theano in the training loop, which is probably inefficient, but whatever.

Now I think everything's working with minibatches and validation

Nope! The network is only outputting 10 values per timestep, maybe it wants the input transposed? Nope that breaks it

Wow, it's taking the softmax across the 10 elements of the minibatch, giving a 10% chance of each note in each, which is wrong. What if I transposed before softmaxing on only multidimensional things put into the softmax function?

Thursday 10/13/16

Not much has happened this week, I made some pretty pictures, learned how to use graphviz and finished the machine learning decision tree homework
I may not get anything done today, Kuperman is out of the office tomorrow and next week is fall break, so my priority is on grad school apps and my resume.

I've modularized out note encodings from training, now I just need to figure it out for generation. I'm going to need new variables, right? Since generation is not (and shouldn't be) minibatched?

Successfully refactored, I'll build product of experts over break some time.

Monday 10/17/16

Building product of experts piece by piece, it's fall break btw and I hope to actually clock in a few hours this week.
I like referring to compile-time and run-time as a priori and a posteriori, idk if that's a real thing

Potential problem later down the line: how do I get the multiple different models to communicate while scanning? I need the outputted probabilities from all of them to make a prediction.
I'm going to have to make a scan that does all of the generative passes in parallel :(

Also TODO: move training loop from generative to a separate file and make it a more general main method, use this to test each individual part of product of experts before combining them

Friday 10/21/16

Tried to modularize the training loop, it's not going to work as well as I had thought, the simple generative and product of experts models want different information as paramters, I need to create wrapper training/validation/testing functions in their classes

Saturday 10/22/16

Finished making wrapper functions in classes, debugged first PoE model,  I'm getting the same loss out of it, which makes me think there's something wrong with the way I'm building the neural net. How can I make things more transparent?

Just for fun, I'm going to make an identity model - the loss will be calculated as the difference between the output and input, any neural net no matter how simple should be able to train on the identity function

That has the same problem

I bet I'm multiplying the probabilities of all of the notes together by accident...
Turns out the way I was doing cost was just totally wrong. I fixed it, now things train!

I also wrote voice contour expert because I'm on a roll today. I'll be back in later, probably, if I have time.

Ooh cool bug - the contour expert had an off by one error, so it was training to predict the current timestep, rather than the next timestep, so the output looked like:
[ 3  4  4  6  6 10 11 11 13  9 12 12 11 11  9  9  4  7  2  0 -3 -5 -5 -5 -5 -4 -6 -8 -6 -3 -5 -2 -2 -2 -2  2  4  6  7  7  7]
changing very gradually

Sunday 10/23/16

I'm going to try to do the measure encoding today
One issue to keep in mind is that the scan functions that I currently have for generative passes aren't going to transfer to the product model, since the random sample needs to be from all of the distributions combined. I'll try to figure out a good way to deal with that
I also would like to make some visualization of what the output of each expert is

Ooh, I should probably take into account the previous timestep in the contour expert for the sake of consistency

Tuesday 10/25/16

I'm off of break now and have some free time before thursday, so I'm going to try to finish up the rhythm expert and figure out how to stop the contour expert from outputting things outside of the midi range
I can't focus, this isn't happening today, I'll work on theory or something.

Thursday 10/27/16

I've found a problem with my initial design for GenerativeLSTM, I don't really handle the case where the input encoding is different from the output encoding, or the only input is the output from the previous timestep, particularly well (or modularly enough to support a rhythm model).
Redesign:
	input encoding is non-recurrent input encoding
	output encoding is output/recurrent input encoding
	if input is None, there's no data coming in, just the previous timestep's output
	if output is None, it is the same as the input encoding and the two similarly encoded things are concatenated

	The input to the network is input_encoding + output_encoding the output is output_encoding

This product model is going to be harder than I thought: I need to circumvent the cost functions in the individual models' training functions and compile different training functions that take into account the product. To do that, I need the input variables and output probability distribution variables for each model to be visible, which feels a bit rough design-wise.
This method leads to another issue - all of the models take different inputs, which it seems correct to derive from a single type of input a priori
My solution is to have the model take a list of strings, rather than a list of instantiated models, as parameter and instantiate all of them internally, using optional parameters for the input theano variable(s)

I gave up on coding for the night and read some papers
https://arxiv.org/pdf/1511.07122v3.pdf - Introduces dilated convolutions for image context aggregation (like identifying which pixels are part of objects in images)
I should ask either Adam or Tom/Alexa to explain some of the math notation involved in defining convolution, this is the technique used in the WaveNet paper

The idea I have going forward is to build a denoising autoencoder http://deeplearning.net/tutorial/dA.html, then use dilated convolution to predict things rather than a recurrent predictive model.

Thursday 11/3/16

Didn't do much this past week, work on PoE model
Now I'm faced with the generation loop for the product model. This is hard, I need to take the product of all of the experts within the body of the scan step function, which means using a for loop with all of the cases -_-

Tuesday 11/8/16

Election day! It's looking like Hillary will win, but still stress (future archaeologists looking back on these research notes will either laugh at my nerves or my naivete)
Another complication arose - I can't pass a 2d list of hidden layer values recurrently into scan, I need to figure out exactly how many hidden layers to deal with everywhere
Figured that out by using a list of partitions, computed a priori
Now there's just a mountain of debugging to do, I'll hopefully come back later today to continue working on it

Thursday 11/10/16

Whelp everything is awful in the world
In terms of honors stuff, though, debugging goes smoothly this morning
It works! Now I'm super super overfitting
I'm instead going to validate on the entire validation set to avoid issues like this

Friday 11/11/16

Current open things to improve:
	output polyphonic music
	modularize multi-expert model and remove some of the magic numbers that made it work
	implement visualizer
	implement test
	write a script to test different architectures, including just regression, each subcombination, the multiple voice spacing models
	pickle weights for easy loading/saving


I did some preliminary experiments(just tried stuff out) and found that spacing + contour is the magic, it reduces error so fast, neither the simple generative nor the rhythm model seem to help much

I'm also looking for more data. This model fits very well, and I'd like to see what it does with a harder problem than bach chorales
I found this big collection of midi: https://www.reddit.com/r/datasets/comments/3akhxy/the_largest_midi_collection_on_the_internet/
it seems too heterogeneous on its own, but I may be able to get something out of it

http://musedata.stanford.edu/ is another possible good source

Monday 11/14/16
The national anthems dataset from the midi collection is a little harder to parse, but I think it might work. Benefit: Voices are split into separate channels based on instrument, though sometimes they overlap. The goal with this dataset is definitely to sound realistic, rather than serve as a score

My plan of action, then, is to get more nice-looking bach midi
or another composer, that would be fine

Challenges here:
	Quantization - can I round midi timestamps to nice numbers?
	More than 4 voices - my neural net architecture should be fine for, say, 18 voices, it just means I need to handle voices dynamically rather than statically

Tuesday 11/15/16

I'm going to write a testing framework that comes up with a visualization of each "expert" and their relative weights, and also graphs loss vs. minibatch number
Then script it to run for a bunch of different subsets of experts and minibatch sizes

I've come to the conclusion that visualization of actual neural network architectures are hard to do, and handwriting graphviz models might just work better

Thursday 11/17/16

On my own time I figured out the website things, I just need to write some content now, I'll deal with that tonight and show Kuperman tomorrow

Now I'm working on the visualization stuff
It works! Wonderful. I have pretty pictures.

Thursday 11/24/16

I'm on a plane home for thanksgiving working on my laptop. When I try to run things over SSH, I get a weird "Illegal action, core dumped" error. I'm not sure if this is a result of trying to run things over SSH (it shouldn't be, right?) or a bug in my code (I haven't tested much yet since finishing PoE).

In order to make things happen on my computer, I changed a bunch of file paths to be relative, not dependent on OCCS directory structure (I should have done this from the start, but whatever)

I also changed the way validation works - instead of randomly choosing four bars from somewhere in each validation piece, I choose a fixed-length prefix of each piece equal to the length of the shortest piece (numpy/theano really dislikes sparse arrays with the way I'm minibatching)

Saturday 11/26

Coming back to campus, what if instead of writing special visualization code for each expert, I just instantiate product of experts with one expert? That seems way better in hindsight.
How I'm getting visualization:

import train
dataset, min_num, max_num, timestep_length = train.load_dataset("../Data/train.p", "../Data/validate.p")
from Models.product_of_experts import MultiExpert
model = MultiExpert(['SimpleGenerative'], 4, 3, min_num, max_num, timestep_length, transparent=True)
train.train(model, 'JustSimpleGenerative', dataset, min_num, max_num, timestep_length, visualize=True)

and such

In order to generate full pieces, I want to change training to train on 2 voice and 3 voice subsets as well as the full 4 voice chorales. To do that, I propose randomly choosing for each sample from the training set, how many voices to take from it, then validate on the validation set with 2, 3 and 4 voices.

I also need a method for saving and loading weights, and a separate program that loads weights for a trained neural net, generates a voice (say, only using the contour expert, since it doesn't require any other data), then iteratively builds up the piece from there. This, I think, is the most interesting generative example because it doesn't depend on any of Bach's music directly.
This program should also have options to generate a voice given one or more of bach's voices for a piece

Monday 1/2/17

Happy New Year! Hopefully 2017 will be better than 2016 (though I'm not hopeful). I've been busy (but not taking notes because I suck) the past few days. Two important things: 
- I found a nasty off-by-one error in the way that I fixed the contour model that confirms that it has learned an identity - so it predicts notes with almost perfect certainty because it was being fed the current timestep as input. After fixing that, the contour outputs look significantly more like what I expected (a gaussian distribution over contours close to zero), but the product model still gets nearly perfect prediction accuracy.
- Someone published a paper doing exactly my project! https://www.csl.sony.fr/downloads/papers/2016/hadjeres-16a.pdf
They do a few things differently, and I'll probably write a blog post about it, basically they use a simple generative model trained forwards and backwards and a non-recurrent network predicting the a note given concurrent notes. Instead of taking the product of those experts, they train another neural network to predict the note given the otuputs of those networks, then use gibbs sampling to generate, rather than doing a single forward pass. They also train on all 12 transpositions of each piece, to avoid doing relative pitch stuff. I don't know which (if any) of those ideas I am interested in using, but this paper really changes the direction my final results should go.

Anyway, I'm going to keep writing retroactive blog posts and catch up with the research I've done so far.

Thursday 1/5/17

Winter Term is here! My plan is to refactor a bunch, then implement training the network using a random prior, then work on including rhythm in the encoding, then build a more robust generation process that can generate entire pieces or start from a single voice of bach and generate from there. Hopefully I can do all of that in a month.

I want to refactor product_of_experts to be recursive so that I can specify nested expert model structures, but generation proves challenging again. I'm going to have to abstract a bunch of things

each model class needs an attribute layers and params of the same length and a function steprec

layers should be a flat list of all of the layers of submodels, including non-recurrent layers
params should be a flat list of all the theano shared variables for the model
steprec should take a bunch of things (see generative.py for the method header) and return the output for a single timestep in terms of internal states and final probabilities (all symbolically, though)


+---------+-------+--------+---------+---------+--------+
| Expert  | Multi | Simple | Spacing | Contour | Rhythm |
+---------+-------+--------+---------+---------+--------+
| Params  |   ✓   |   ✓    |    ✓    |    ✓    |    ✓   |
| Layers  |   ✓   |   ✓    |    ✓    |    ✓    |    ✓   |
| steprec |   ✓   |   ✓    |    ✓    |    ✓    |    ✓   |
+---------+-------+--------+---------+---------+--------+

Monday 1/9/17

I continued refactoring a bit and made it so that models have to be instantiated and passed in a list to MultiExpert to allow for varied parameters
This is harder than expected, because of theano input things, I wonder what I need to pass into each expert model for it to work? Spoiler: it's the shared variables

Tuesday 1/10/17

Solved that issue, I think all of this works now. I just have to rewrite the function that easily instantiates models, but I'll do that later. Next objective: add option for random priors

After consulting the literature a little more, I'm unsure where I got the idea for random numbers in the input vector from. Rather than just include them, I am unsure how many I'm supposed to have and what values they're supposed to take. Ideally the model would learn that they don't have any bearing on the output, so all they can do is provide obfuscation for the actual function I'm trying to model. Maybe that's the wrong idea. I'll ask Ben. In the meantime, working on a weight saving method should be useful now. My plan is to start with pickle files, then look into more space efficient methods.

Pickle is super easy to use. End of story. I'll start the generative stuff tomorrow.

Thursday 1/12/17

I want a script that loads a trained model pickle file, runs generation from scratch. What does that entail?

The first voice can't be generated using the spacing expert, and the first note can't be generated using the contour expert.
This makes me want to have a separate model trained to predict each voice -- a bass model that doesn't use spacing, then soprano alto and tenor models that use different amounts of spacing information

An alternate approach, inspired by the gibbs sampling of the sony paper is to generate each voice without respect to spacing, then repeatedly regenerate each voice using a spacing-sensitive model until it converges.

I think I also want to try to do the gibbs sampling by modularizing my approach to allow for the training of backwards models and take the product of a forwards and backwards multiexpert

Well, that didn't happen today. I did fix a bug where I had hard coded in the known voice for the spacing model, so there's that.

Sunday 1/15/17

Worked on instantiating models for a bit today, there's really no way to do this modularly, so I'm just coding in all of the specifications (i.e. bass is just generated by contour and beat models, soprano comes next based on just spacing with bass, contour and beat info, etc. for alto and tenor). The upside is that generating for this method should be pretty easy.

Monday 1/16/17

Finished writing the training code, started to test. I still need to come up with a function that can translate a multi-voice piece into a midi file. I'm unsure how midi handles multiple channels, I need to look into whether I should do that or combine them all on one channel like in the training data.

using dill instead of pickle so that I can pickle objects that have lambda expressions as attributes

Tuesday 1/17/17

Past Sam already wrote a multi-voice timestep-to-midi converter. Thanks!

And I have full midi output. This is exciting, although it doesn't sound particularly good at all. I haven't paid much attention to rhythm at all, and most of the voices change pitch every timestep. This is bad. I wonder if I change my timestep encoding to have beat information, things will sound better?

Ok, here's my current idea: Create a separate, unrelated neural net that predicts rhythm (articulate, sustain or rest) for each voice. This makes the naming of the rhythm expert awfully confusing, but whatever. This sort of thing makes generation confusing as ever, since now every model will have to listen to this new rhythm model when sampling to figure out what to see next. I wonder if I should include bits for sustain, articulate and rest in the input encoding too?

There's another problem here, the training data didn't have encodings for sustain/articulate. Should I just annotate it with a sustain if the note doesn't change and an articulate if it does? I think that would be pretty easy to implement, but I'm unsure how true to the source material it would be (Bach often wrote rearticulated notes to allow voices to have the same lyrics). I'm considering using Music21 (the wonderful library from MIT for music information retrieval), which has all of the same chorales stored as MusicXML files and comes with a handy parser. I think I can make that work too if I want this encoding to work.

I'm going to do two other generation schemes first (they should be fine if I change the dataset since they're so far abstracted) and then try the music21 thing.

Done with the generation schemes, they all sound awful, but I think figuring out the rhythm encoding will help with that. Trying to install music21's visualizer (which would be really useful) depends on musescore, which depends on a newer version of cmake, which I'm trying to compile from source.

This is awful and I'm not going to try to debug the segfault cmake is giving me. I'll just avoid music21's visualization system.

Saturday 1/21/17

I spent most of this week reading Xenakis and applying for jobs. Tonight I finished up the music21 dataset to timestep converter
It was super smooth, since my code was modular enough. Now I'm running a test run of the model. Training is a lot slower, since I also increased the minibatch size from two bar excerpts to four bar and from 10 pieces to 20, but that has nothing to do with the new dataset (which should contain the same pieces). I'm going to leave it training overnight and see what happens.

Sunday 1/22/17

Now that I've got the music21 dataset loader written, I can test with a palestrina dataset as well if I want to. I also want to implement the articulate/sustain/rest model

Articulation data is in vectors of length 3 [articuate, sustain, rest] for each timestep for each voice. The dimensions of articulation_info returned by the function that parses the music21 datasets is [piece, voice, timestep, articulation]

The palestrina dataset (which I'm kind of looking at in parallel with articulation stuff) is of varied numbers of voices (since it was the renaissance), mostly 4-7. I'd need to find some way to differentiate between voices for training (i.e. a voice model shouldn't train on tenor in one piece and alto 2 in another)

It turns out integrating the articulation model into the MultiExpert architecture might be a bit tricky. I'm going to have to make the top-level Multi expert in any architecture have its own articulation model that it uses for generation only (trained separately from the product model that has its own whole list of parameters)

Monday 1/23/17

Well, Ben just resigned from the department, so the future of this project is kind of in jeopardy. We'll see if I can get reassigned and finish my project. For now, I'm just going to keep trying to get the articulation model working. I also didn't get the google job, so that's that.

As with any modification to my call to scan, things are messy, so we'll see if I can get it to work. 

I think it's working now, I still need to encorporate the articulation in the way I'm creating midi files, though.

Wednesday 1/25/17

The timesteps_to_notes modifications I had to make were pretty easy, but they reveal another problem that I think is with onehot to int

Ooh, embarrasing error in onehot to int! I was creating a timestep with pitch -1 every time, often after a real timestep, since I tried to use break to break out of two for loops at once. This might change everything with regard to my output! We'll see.

I'm realizing that two things I am doing while training are questionable: first, I train the two models simultaneously, which means that one might overfit while the other is still getting there. Second, I train the pitch model on timesteps where its output might not be used. I really need to use the probability of the articulation model articulating as a multiplier on the chances of the pitch model's output being used when calculating error. Yeah, actually, how do I calculate error when the training data is a rest?? Since I'm using the training data as a mask, it just won't add anything to the log liklihood. That should change.

This output sounds waaaaay better. Funny how bug fixes work. Anyway, the midi files I'm outputting still have a lot of note on/off signals that should be long notes. I'm curious if that's another bug or if I'm just outputting articulate every timestep. If I am, that's bad.

I'm always outputting articulate. Huh. After investigating the training data, I was actually never setting any of the articulation data to sustain, so that makes sense. I fixed the bug (I was never actually setting anything when loading the dataset, so there was no data for most timesteps in the articulation data), so now it should train better.

Well, now the articulation model is only outputting sustain. That's pretty bad and probably means I need to rethink the way I build the articulation model. I'm going to try to revert the switch to articulation models and fix the issues with timesteps to midi again to see whether I get similar gains. Thank god (and Linus) for git. The articulation model might just be unnecessary.

I think I've now done that, fixed timesteps to notes and all the related functions.

I'm going to figure out a way to visualize generation and use abjad to make scores out of the output without going through midi.

Thursday 1/26/17

I left the chorale generation on overnight, and it turned out awful again. I am unsure what exactly models are learning so well that they are unable to reproduce when not given a sample to generate from. 

Hold on, I'm getting the piece to generate from from the training set. That's fishy. I'm going to change it to the validation set and see if I get output that sounds as nice. The other idea I've got is to do generation in several voices simultaneously, which would take some rewriting of the generation scripts, but is probably doable. Well, how do I figure out spacing? What if I had a new model, where one model predicts each of the 6 different spacings and I have another for contour/rhythm for each voice? How would I take the product? I'd need to know the offset for that voice at that time. This is interesting math. I'd need the spacing models to interact in such a way that I combine the output of soprano/alto + alto/tenor with soprano/tenor. That said, this still feels like boostrapping. At what point do I get real pitch values? One voice needs to be set in stone. Or I need to do the Gibbs sampling thing. What if I regenerated voices instead of regenerating individual notes? That seems computationally simpler that doing effectively an entire generative run for each note multiple times.

I'm going to try that since it's not too hard.

While that's running, I'm going to look into visualizing generation through heatmaps and scores.

Transparent generation would have to have steprec for everything return a probability distribution and have step in multiexpert return a 2d list of probabilities for each timestep, then generate would have to reshape them and return a 3d list of [sub-model, timestep, probability], then I could visualize it.

Friday 1/27/17

How does everything sound this bad??? It is doing so well generating based on the validation set, which it hasn't even seen before! Maybe I should train it on garbage other voices as well as good ones? No, but then it'll just learn bad things.

For my own sanity, I tried generating each voice as if it had been put in the original piece. It still sounds awful. When it's unsure what note should go somewhere, it just kind of trills between the two (the network was probably outputting equal probability for each one). It really wants to have the articulation model, just the articulation data needs to be part of the output of the model as is, which means I need to do the partial softmax thing.

Tuesday 1/31/17

The US government is on the verge of collapse, or so it seems. In terms of implementation details, this encoding switch may prove difficult, but I think I have to try it. Why didn't I use a library like Keras from the start?? It's so much easier to set up... :(

Articulation TODO:
VoiceContour
Rhythm
Multiexpert
train.py - probably finished?? I did timesteps to notes and load_dataset_music21, load_dataset doesn't support the timestep model, so it's now deprecated
chorale_generator.py

I've made the decision that the encoding size passed into the generative model will include the articulation bits, since it requires rewriting less code

test:
from train import *
file_list = music21.corpus.getBachChorales()
dataset, min_num, max_num, timestep_length = load_dataset_music21(file_list)
model = SimpleGenerative(max_num-min_num+3, 3, [100,200,100], 4, 3)
train(model, 'SimpleGenerative', dataset, min_num, max_num, timestep_length, output_dir=None, visualize=False)


Wednesday 2/1/17

How should other models implement the articulation encoding? Should the spacing model get as input the known voice's articulation or the unknown voice's? I think they should all just get the previous timestep's info from the voice they're predicting

from train import *
dataset, min_num, max_num, timestep_length = pickle.load(open('../Data/music21articulation.dataset', 'rb'))
model = VoiceSpacingExpert(max_num-min_num+3, 3, [100,200,100], 0, 3)
train(model, 'VoiceSpacingExpert', dataset, min_num, max_num, timestep_length, output_dir=None, visualize=False)

I found an issue with voice spacing expert's system of translating from spacing to absolute pitches - the way I encode spacing wraps around negative values, but when I decode, I treat those as positive values. That means no unknown voice can be below a known voice. I'm going to solve that by adding half the encoding size to the values before converting them to onehot in training, then subtracting half the encoding size when converting back during generation. This is going to change steprec as well.

Now for some reason it's always outputting 'rest'. I think I might have been softmaxing across the wrong axis, since it's no longer a softmax layer but a softmax function. Now that that's fixed, we actually train.
Now I'm still getting a lot of rests because it's outputting sustain every timestep and if it sustains the first timestep, the result is a sustained rest. Why is it learning to sustain with almost 100% certainty? at least 10% of the training data is articulate.

I honestly can't figure out what to do. The model keeps getting stuck in the minimum where it always outputs sustain because it usually is sustain. Older models have done the "note length" prediction model separately from the pitch one. That might be an option here. I'll ask Adam tomorrow.

Friday 2/3/17

Adam gave the pretty good suggestion that I should try the articulation expert again, but with a more balanced training set (i.e. don't train it with the same minibatches as the pitch experts, use a different sort of dataset that has more articulation timesteps and fewer sustain ones), so I'm going to try that out today. I'm also going to change the articulation model to have the timestep data as input.

The issue of how to select training data becomes tricky. Ideally, I want to show the model all of the timesteps, so that it gets the right hidden states, but only update on some of them. What that means is that I need to select some timesteps to be involved in the loss calculation and some to not, so that outputting "sustain" every time is not better than outputting "articulate" with some inaccuracy according to patterns in training data. My best guess for that seems to be choosing some timesteps where the correct answer is sustain and multiplying their loss by 0, but I'm not sure how to do that in the minibatch setup I have.

What if I add an additional parameter to my training function called "training_mask", then multiply the correct training data by the mask to determine if I train by it?

Well, that seems to make it prefer articulating greatly. I want to actually look at articulate loss vs. sustain loss and see that breakdown.

Saturday 2/4/17

The output seems better, but now the changes to the voice spacing model are giving me issues. I really need a full note encoding for positive and negative distances, since spacings can be more than 1/2 of the total encoding reasonably often.

Sunday 2/5/17

There seems to also be a problem with the way I'm getting out contour probs to the product model. I think I should figure out where the min_num is relative to the "zero" of the relative encoding

in the pdf of the contour output, next_note[max_num-prev_note] (both in midi numbers) is the min_num probability and next_note[2 * max_num - min_num - prev_note] is the max_num probability, assuming that next_note[max_num-min_num] is the prev_note probability

**algebra!**

The spacing output should be the same math, since it's also dealing with a relative pitch that can be above or below the predicted pitch

so how do I slice different subarrays according to different start and end points? I think I have to do a map and then stack

this math is probably wrong... I'm getting values that are higher than max_num, I'm going to go home and rest for a while then try again.

I went home and showered and rested a bit, and it makes sense now: min_num = next_note[max_num-min_num-prev_note] because prev_note is already in reference to min_num, it's a one-hot encoding

Now it's working, but the intervals are still really big. The notes make sense, so I'm unsure what's up. I think I'll work on the generation visualizer now and see if that helps.

Lol I wasn't using all the experts yet. I'll check what happens now.

Monday 2/6/17

It's outputting more music, the rhythm experts definitely made it sound a lot more consistent, but the individually-rhythmically appropriate voices are out of sync with each other. What if my articulation expert had other voices' articulation as input as well?

Wednesday 2/8/17

That was super easy to implement and seems to help? I don't really know at this point.

I also had the idea to use the same training set dropout system on the pitch models -- only train on timesteps that articulate -- so that the model is not learning a lot for timesteps it's not needed for. I did that using the same system, but am now figuring out the articulation dataset in both the pitch and articulation training functions. If I end up refactoring, this is a good place to cut code and time by merging the two and doing the halting process within the product of experts model function.

This also means that train.py has abandoned compatability with any model except for the product model, which is fine because you can always have a product of one, but should probably be documented somewhere.

Thursday 2/9/17

Lol when I switched to the music21 dataset, I didn't think to make sure 0 was still bass and 3 was still soprano. I've been naming my models backwards this whole time. 

Monday 2/13/17

I have an outline for my paper and a draft survey to administer now, so that part of the project, at least, is going as planned.
I'm trying two things, one kind of weird, one not: I'm experimenting leaving out the softmax layers in the middle of a multiexpert, so the outputs from the expert models are not forced to fit a probability distribution. This might mess up the product weighting horribly, but also might allow some experts to *dislike* certain notes as much as other experts like them. If it doesn't work, it'll be super easy to switch back, so it seems fine.
Second, I'm experimenting with lower sustain dropout in the articulation model. I noticed the opposite problem to what I had before, too many articulation timesteps, so maybe 50% was too much.

no softmax does not work, weighted product does not work with zero or negative numbers. I would need to have another neural network layer combine them, and I'm not sure how to do that.

increasing minibatch size also doesn't seem to work because there is one piece in the training set that is too short, so I removed that peice

Tuesday 2/14/17

Valentines day! I'm looking at the output and I'm at the point at which each voice seems pretty great. I have no idea how to extend that to the multi-voice generation output, though. One thought I've had is that the simple and rhythm experts are affecting the output too much, so it's choosing notes based on range and tonal consistency with themselves, not with the other voices, which is what the spacing models are supposed to do. I'm going to try just taking the product of all the experts, without the nested spacing multiexpert, and see what that does.

Wednesday 2/15/17

The lack of a spacing multiexpert didn't seem to help or hurt too much, it places much more emphasis on the generative model. Even when I take the generative model out, the spacing models just get deemphasized by the product.

Wednesday 2/22/17

Lots of work and not lots of notes! I wrote a draft introduction to my thesis which is probably way too long. I feel like Mozart, spending way too much time on my exposition I won't have the minutes to devote to my development that I'd like. Wow that sounds pretentious...eh

I've also got the survey I wrote approved by the IRB, so I'm going to talk to Adam about setting up the qualtrics stuff and administering it, possibly starting tomorrow!

Thursday 2/23/17

I decided to choose samples from the output at random -- I'm going to choose three samples from each of eight categories - bach melody, bach harmony, simple melody, simple harmonization, simple harmony, multi melody, multi harmonization, multi harmony

The numbers I randomly got are as follows:

array([[6, 3, 9],
       [4, 9, 6],
       [0, 7, 2],
       [8, 4, 0],
       [7, 1, 9],
       [1, 2, 3],
       [5, 9, 1],
       [7, 6, 8]])

stuff's running on sinclair.oberlin.edu since it really doesn't like to do things via ssh for some reason

Saturday 2/25/17

Uploading things to soundcloud

samples
1,2,3 		- simple melody
4,5,6 		- multi melody
7,8,9 		- bach melody
10,11,12	- simple harmonization
13,14,15 	- multi harmonization
16,17,18	- bach full
19,20,20 	- simple full
21,22,23	- multi full

Thursday 3/2/17

Survey is going well so far, I'm going to leave it up until Monday, I've already exceeded my goals for sample size though. I also gave samples to Leydon to analyze, the email I sent her is as follows:

"Hi Dr. Leydon,

Thank you again for agreeing to help with this part of my project! Attached are four short compositions. Recordings (synthesized from midi files) are at the links provided below.

I'd like you to answer the following questions in a sentence or two for each sample, treating them as though they were the product of a novice (human) composer, explaining your answers with examples when possible:

1. To what extent does this piece demonstrate domain competence? (i.e. is it recognizably a chorale, does it follow the conventions of chorale composition, etc.)

2. To what extent does this piece demonstrate the intentionality, investment or emotional expression of the composer?

3. To what extent does this piece demonstrate original thinking or come across new relationships between existing concepts?

4. To what extent is this piece valuable as a piece of music or work of art?

I am interested more in the "why" than the evaluation (don't worry about my personal feelings or the feelings of the composer). I know how these pieces sound and I want your honest opinions.

If you can do this in the next week or so, that would be amazing. If not, I do need it before the start of spring break (or, if necessary, in the first day or two of spring break). My paper is due at the end of break, and I'm depending on your input to make conclusions.

Thank you so much! Let me know if there are any issues.

Sam"


While I'm at it, I'll include the message that was sent out to various mailing lists accompaning the survey:

"Hi, I am currently conducting a survey as part of my honors research in computer science. If you choose to participate, you'll be asked to listen to some music samples and give your opinions on them, it's completely anonymous and will take about 8-10 minutes. Thank you for your help!"

I also posted the survey on Facebook with this text:

"I did research making computers compose music! You should take my survey, listen to music samples and guess if those samples were composed by a human! If lots of people do, I might have a stastically significant sample size...

If you choose to participate, you'll be asked to listen to some music samples and give your opinions on them, it's completely anonymous and will take about 8-10 minutes. The survey is open until next Monday.
Thank you for your help!"

While I'm not sure who exactly has taken the survey, it was shared by some Oberlin alums and some of my family members, so while it has a broader sample than just Oberlin students, it is very much constrained to people two or three degrees of separation from me. Further, the people who felt inclined to take the survey were mostly musicians

3/12/17

Not a lot of notes recently! I did the survey, it finished, I'll analyze the results at some point. Stats is hard, graphs are easy.

My probability output for the multiexpert was misnumbered, so the "multi-expert output" was actually the beat expert output, hopefully that's fixed now and the output I'm getting to show graphs of training rates for the two models will serve that purpose in my paper.