Research Notes:

I plan to take notes pretty often on what I'm doing with honors.

Sunday, 9/11/16

The second week of the semester is over now. I didn't do anything the first week, and the second week I made a schedule and started working on midi_parser.py
I'm now realizing that my dataset from UMontreal does not have note on/off for the same pitch if the voice switches. This makes sense, since it reduces the size of the midi file without impacting the sound at all, but it's bad for me because I need to split those notes up somehow. My algorithm with voice crossing and voice overlap not allowed works, though, so as soon as I solve this problem, I'll be done cleaning the data I've got.

Tuesday 9/13/16

More bug fixes to midi_parser.py. Hopefully, I can finish this up by tomorrow/thursday and start on real ML

Wednesday 9/14/16

Works! A few edits to the way I was setting upper/lower bounds made it guess correctly on examples most of the time, when it doesn't know, though, I should make it choose closest voice not lowest

Thursday 9/15/16

I messed up huge. While opening parse_midi.py, I saw there was a swap file and overwrote whatever was in the swap file without really thinking. That was an early version of the program that didn't do anything, I lost all of my work. I got some of it back using the parse_midi.py~ file that vim creates as a backup, but that was still from Tuesday. I lost a few hours of work. I'm going to try to get all of that done again today.

I managed to fix all of the lost stuff, but now I'm running into a problematic passage that no amount of algorithm can fix. I've got a crazy idea: when the rules don't specify, randomly choose a voice and continue - that way it will probably get the right answer at some point.

Friday 9/16/16

I did a bit of reorganization today and also explored additional training sets, I just want to account for everything while writing code for models. I'm currently looking at Music21, a library for music information retrival from MIT that has a bunch of bach and palestrina and other stuff that I might find useful.

Saturday 9/17/16

I started working on the generative model today, and quickly realized I was unsure what I wanted to accomplish. The easy answer is to say that I want to try a bunch of things, but I'm not sure. For now, I'm going with the idea that given the previous timestep and the current timestep in some number of voices, I want to predict the next time step of another voice.

Wednesday 9/21/16

I kept working on the generative model a bit, and read a cool article by Anna Jordanous on evaluating computational creativity research.

Thursday 9/29/16

Sorry for few notes the past few days, I didn't do much work, mostly debugging the generative model (I spent way too long on a bug that ended up being order of operations (a + b * c != (a+b) * c). I also got word back from the two people I had emailed about grad school stuff, they both basically told me to keep shopping around the way I had (and also talk to Bob Keller, which I did). I'm going to let the grad school stuff rest for a week while I get back on track with honors work.

Other than that, the GRE went well and I'm on my way. If I want to sample from a custom distribution over discrete categories, how do I do that? Never mind, it was in some old code I had written, rng.choice lets you pass in a distribution. Now I'm trying to figure out a mismatched type error

Nope, I called the wrong function.

Got the model working! It produces output! I'm going to work on a way to output midi instead of midi numbers.

And I did that, what a productive evening! I'm going home.

Friday 9/30/16

It did the thing! I have results from the simple generative model. They're not very good.

TODO:
Mini-batch training seems like a must
Validation to prevent over-fitting
Refactor to allow for modular note encodings
Product-of-experts (rhythm, pitch, contour)

Future:
Autoencoders?
Convolution across time? (a la WaveNet)

If I look at the loss function over time, it's not training really at all. I think I messed something up, but I don't know what.

Wednesday 10/5/16

Time to refactor and make sure everything is ok

minibatches - each piece needs to be the same length
my plan: randomly choose pieces, pick 8-timestep segments from each piece chosen at random

This stuff is done now, but everything's buggy.

I need to learn how convolution actually works - what would a one-dimensional convolution with kernel of size 1xk mean? Maybe I should ask Adam

Anyway, I also read the pixel-rnn article. The math is a bit confusing, but I get the gist of what they are doing.

Thursday 10/6/16

Now I'm getting to the meat of the problem: I'm not storing 2 dimensions of internal state

Should a section actually snag the timestep before as well, so it can feed that in rather than zeros? How can I do that while still allowing the start of a piece to be used?
Figured it out, doing all of that as processing outside of theano in the training loop, which is probably inefficient, but whatever.

Now I think everything's working with minibatches and validation

Nope! The network is only outputting 10 values per timestep, maybe it wants the input transposed? Nope that breaks it

Wow, it's taking the softmax across the 10 elements of the minibatch, giving a 10% chance of each note in each, which is wrong. What if I transposed before softmaxing on only multidimensional things put into the softmax function?

Thursday 10/13/16

Not much has happened this week, I made some pretty pictures, learned how to use graphviz and finished the machine learning decision tree homework
I may not get anything done today, Kuperman is out of the office tomorrow and next week is fall break, so my priority is on grad school apps and my resume.

I've modularized out note encodings from training, now I just need to figure it out for generation. I'm going to need new variables, right? Since generation is not (and shouldn't be) minibatched?

Successfully refactored, I'll build product of experts over break some time.

Monday 10/17/16

Building product of experts piece by piece, it's fall break btw and I hope to actually clock in a few hours this week.
I like referring to compile-time and run-time as a priori and a posteriori, idk if that's a real thing

Potential problem later down the line: how do I get the multiple different models to communicate while scanning? I need the outputted probabilities from all of them to make a prediction.
I'm going to have to make a scan that does all of the generative passes in parallel :(

Also TODO: move training loop from generative to a separate file and make it a more general main method, use this to test each individual part of product of experts before combining them

Friday 10/21/16

Tried to modularize the training loop, it's not going to work as well as I had thought, the simple generative and product of experts models want different information as paramters, I need to create wrapper training/validation/testing functions in their classes

Saturday 10/22/16

Finished making wrapper functions in classes, debugged first PoE model,  I'm getting the same loss out of it, which makes me think there's something wrong with the way I'm building the neural net. How can I make things more transparent?

Just for fun, I'm going to make an identity model - the loss will be calculated as the difference between the output and input, any neural net no matter how simple should be able to train on the identity function

That has the same problem

I bet I'm multiplying the probabilities of all of the notes together by accident...
Turns out the way I was doing cost was just totally wrong. I fixed it, now things train!

I also wrote voice contour expert because I'm on a roll today. I'll be back in later, probably, if I have time.

Ooh cool bug - the contour expert had an off by one error, so it was training to predict the current timestep, rather than the next timestep, so the output looked like:
[ 3  4  4  6  6 10 11 11 13  9 12 12 11 11  9  9  4  7  2  0 -3 -5 -5 -5 -5 -4 -6 -8 -6 -3 -5 -2 -2 -2 -2  2  4  6  7  7  7]
changing very gradually

Sunday 10/23/16

I'm going to try to do the measure encoding today
One issue to keep in mind is that the scan functions that I currently have for generative passes aren't going to transfer to the product model, since the random sample needs to be from all of the distributions combined. I'll try to figure out a good way to deal with that
I also would like to make some visualization of what the output of each expert is

Ooh, I should probably take into account the previous timestep in the contour expert for the sake of consistency

Tuesday 10/25/16

I'm off of break now and have some free time before thursday, so I'm going to try to finish up the rhythm expert and figure out how to stop the contour expert from outputting things outside of the midi range
I can't focus, this isn't happening today, I'll work on theory or something.

Thursday 10/27/16

I've found a problem with my initial design for GenerativeLSTM, I don't really handle the case where the input encoding is different from the output encoding, or the only input is the output from the previous timestep, particularly well (or modularly enough to support a rhythm model).
Redesign:
	input encoding is non-recurrent input encoding
	output encoding is output/recurrent input encoding
	if input is None, there's no data coming in, just the previous timestep's output
	if output is None, it is the same as the input encoding and the two similarly encoded things are concatenated

	The input to the network is input_encoding + output_encoding the output is output_encoding

This product model is going to be harder than I thought: I need to circumvent the cost functions in the individual models' training functions and compile different training functions that take into account the product. To do that, I need the input variables and output probability distribution variables for each model to be visible, which feels a bit rough design-wise.
This method leads to another issue - all of the models take different inputs, which it seems correct to derive from a single type of input a priori
My solution is to have the model take a list of strings, rather than a list of instantiated models, as parameter and instantiate all of them internally, using optional parameters for the input theano variable(s)

I gave up on coding for the night and read some papers
https://arxiv.org/pdf/1511.07122v3.pdf - Introduces dilated convolutions for image context aggregation (like identifying which pixels are part of objects in images)
I should ask either Adam or Tom/Alexa to explain some of the math notation involved in defining convolution, this is the technique used in the WaveNet paper

The idea I have going forward is to build a denoising autoencoder http://deeplearning.net/tutorial/dA.html, then use dilated convolution to predict things rather than a recurrent predictive model.

Thursday 11/3/16

Didn't do much this past week, work on PoE model
Now I'm faced with the generation loop for the product model. This is hard, I need to take the product of all of the experts within the body of the scan step function, which means using a for loop with all of the cases -_-