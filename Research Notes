Research Notes:

I plan to take notes pretty often on what I'm doing with honors.

Sunday, 9/11/16

The second week of the semester is over now. I didn't do anything the first week, and the second week I made a schedule and started working on midi_parser.py
I'm now realizing that my dataset from UMontreal does not have note on/off for the same pitch if the voice switches. This makes sense, since it reduces the size of the midi file without impacting the sound at all, but it's bad for me because I need to split those notes up somehow. My algorithm with voice crossing and voice overlap not allowed works, though, so as soon as I solve this problem, I'll be done cleaning the data I've got.

Tuesday 9/13/16

More bug fixes to midi_parser.py. Hopefully, I can finish this up by tomorrow/thursday and start on real ML

Wednesday 9/14/16

Works! A few edits to the way I was setting upper/lower bounds made it guess correctly on examples most of the time, when it doesn't know, though, I should make it choose closest voice not lowest

Thursday 9/15/16

I messed up huge. While opening parse_midi.py, I saw there was a swap file and overwrote whatever was in the swap file without really thinking. That was an early version of the program that didn't do anything, I lost all of my work. I got some of it back using the parse_midi.py~ file that vim creates as a backup, but that was still from Tuesday. I lost a few hours of work. I'm going to try to get all of that done again today.

I managed to fix all of the lost stuff, but now I'm running into a problematic passage that no amount of algorithm can fix. I've got a crazy idea: when the rules don't specify, randomly choose a voice and continue - that way it will probably get the right answer at some point.

Friday 9/16/16

I did a bit of reorganization today and also explored additional training sets, I just want to account for everything while writing code for models. I'm currently looking at Music21, a library for music information retrival from MIT that has a bunch of bach and palestrina and other stuff that I might find useful.

Saturday 9/17/16

I started working on the generative model today, and quickly realized I was unsure what I wanted to accomplish. The easy answer is to say that I want to try a bunch of things, but I'm not sure. For now, I'm going with the idea that given the previous timestep and the current timestep in some number of voices, I want to predict the next time step of another voice.

Wednesday 9/21/16

I kept working on the generative model a bit, and read a cool article by Anna Jordanous on evaluating computational creativity research.

Thursday 9/29/16

Sorry for few notes the past few days, I didn't do much work, mostly debugging the generative model (I spent way too long on a bug that ended up being order of operations (a + b * c != (a+b) * c). I also got word back from the two people I had emailed about grad school stuff, they both basically told me to keep shopping around the way I had (and also talk to Bob Keller, which I did). I'm going to let the grad school stuff rest for a week while I get back on track with honors work.

Other than that, the GRE went well and I'm on my way. If I want to sample from a custom distribution over discrete categories, how do I do that? Never mind, it was in some old code I had written, rng.choice lets you pass in a distribution. Now I'm trying to figure out a mismatched type error

Nope, I called the wrong function.

Got the model working! It produces output! I'm going to work on a way to output midi instead of midi numbers.

And I did that, what a productive evening! I'm going home.

Friday 9/30/16

It did the thing! I have results from the simple generative model. They're not very good.

TODO:
Mini-batch training seems like a must
Validation to prevent over-fitting
Refactor to allow for modular note encodings
Product-of-experts (rhythm, pitch, contour)

Future:
Autoencoders?
Convolution across time? (a la WaveNet)

If I look at the loss function over time, it's not training really at all. I think I messed something up, but I don't know what.

Wednesday 10/5/16

Time to refactor and make sure everything is ok

minibatches - each piece needs to be the same length
my plan: randomly choose pieces, pick 8-timestep segments from each piece chosen at random

This stuff is done now, but everything's buggy.

I need to learn how convolution actually works - what would a one-dimensional convolution with kernel of size 1xk mean? Maybe I should ask Adam

Anyway, I also read the pixel-rnn article. The math is a bit confusing, but I get the gist of what they are doing.

Thursday 10/6/16

Now I'm getting to the meat of the problem: I'm not storing 2 dimensions of internal state

Should a section actually snag the timestep before as well, so it can feed that in rather than zeros? How can I do that while still allowing the start of a piece to be used?
Figured it out, doing all of that as processing outside of theano in the training loop, which is probably inefficient, but whatever.

Now I think everything's working with minibatches and validation

Nope! The network is only outputting 10 values per timestep, maybe it wants the input transposed? Nope that breaks it

Wow, it's taking the softmax across the 10 elements of the minibatch, giving a 10% chance of each note in each, which is wrong. What if I transposed before softmaxing on only multidimensional things put into the softmax function?

Thursday 10/13/16

Not much has happened this week, I made some pretty pictures, learned how to use graphviz and finished the machine learning decision tree homework
I may not get anything done today, Kuperman is out of the office tomorrow and next week is fall break, so my priority is on grad school apps and my resume.

I've modularized out note encodings from training, now I just need to figure it out for generation. I'm going to need new variables, right? Since generation is not (and shouldn't be) minibatched?

Successfully refactored, I'll build product of experts over break some time.

Monday 10/17/16

Building product of experts piece by piece, it's fall break btw and I hope to actually clock in a few hours this week.
I like referring to compile-time and run-time as a priori and a posteriori, idk if that's a real thing

Potential problem later down the line: how do I get the multiple different models to communicate while scanning? I need the outputted probabilities from all of them to make a prediction.
I'm going to have to make a scan that does all of the generative passes in parallel :(

Also TODO: move training loop from generative to a separate file and make it a more general main method, use this to test each individual part of product of experts before combining them

Friday 10/21/16

Tried to modularize the training loop, it's not going to work as well as I had thought, the simple generative and product of experts models want different information as paramters, I need to create wrapper training/validation/testing functions in their classes

Saturday 10/22/16

Finished making wrapper functions in classes, debugged first PoE model,  I'm getting the same loss out of it, which makes me think there's something wrong with the way I'm building the neural net. How can I make things more transparent?

Just for fun, I'm going to make an identity model - the loss will be calculated as the difference between the output and input, any neural net no matter how simple should be able to train on the identity function

That has the same problem

I bet I'm multiplying the probabilities of all of the notes together by accident...
Turns out the way I was doing cost was just totally wrong. I fixed it, now things train!

I also wrote voice contour expert because I'm on a roll today. I'll be back in later, probably, if I have time.

Ooh cool bug - the contour expert had an off by one error, so it was training to predict the current timestep, rather than the next timestep, so the output looked like:
[ 3  4  4  6  6 10 11 11 13  9 12 12 11 11  9  9  4  7  2  0 -3 -5 -5 -5 -5 -4 -6 -8 -6 -3 -5 -2 -2 -2 -2  2  4  6  7  7  7]
changing very gradually

Sunday 10/23/16

I'm going to try to do the measure encoding today
One issue to keep in mind is that the scan functions that I currently have for generative passes aren't going to transfer to the product model, since the random sample needs to be from all of the distributions combined. I'll try to figure out a good way to deal with that
I also would like to make some visualization of what the output of each expert is

Ooh, I should probably take into account the previous timestep in the contour expert for the sake of consistency

Tuesday 10/25/16

I'm off of break now and have some free time before thursday, so I'm going to try to finish up the rhythm expert and figure out how to stop the contour expert from outputting things outside of the midi range
I can't focus, this isn't happening today, I'll work on theory or something.

Thursday 10/27/16

I've found a problem with my initial design for GenerativeLSTM, I don't really handle the case where the input encoding is different from the output encoding, or the only input is the output from the previous timestep, particularly well (or modularly enough to support a rhythm model).
Redesign:
	input encoding is non-recurrent input encoding
	output encoding is output/recurrent input encoding
	if input is None, there's no data coming in, just the previous timestep's output
	if output is None, it is the same as the input encoding and the two similarly encoded things are concatenated

	The input to the network is input_encoding + output_encoding the output is output_encoding

This product model is going to be harder than I thought: I need to circumvent the cost functions in the individual models' training functions and compile different training functions that take into account the product. To do that, I need the input variables and output probability distribution variables for each model to be visible, which feels a bit rough design-wise.
This method leads to another issue - all of the models take different inputs, which it seems correct to derive from a single type of input a priori
My solution is to have the model take a list of strings, rather than a list of instantiated models, as parameter and instantiate all of them internally, using optional parameters for the input theano variable(s)

I gave up on coding for the night and read some papers
https://arxiv.org/pdf/1511.07122v3.pdf - Introduces dilated convolutions for image context aggregation (like identifying which pixels are part of objects in images)
I should ask either Adam or Tom/Alexa to explain some of the math notation involved in defining convolution, this is the technique used in the WaveNet paper

The idea I have going forward is to build a denoising autoencoder http://deeplearning.net/tutorial/dA.html, then use dilated convolution to predict things rather than a recurrent predictive model.

Thursday 11/3/16

Didn't do much this past week, work on PoE model
Now I'm faced with the generation loop for the product model. This is hard, I need to take the product of all of the experts within the body of the scan step function, which means using a for loop with all of the cases -_-

Tuesday 11/8/16

Election day! It's looking like Hillary will win, but still stress (future archaeologists looking back on these research notes will either laugh at my nerves or my naivete)
Another complication arose - I can't pass a 2d list of hidden layer values recurrently into scan, I need to figure out exactly how many hidden layers to deal with everywhere
Figured that out by using a list of partitions, computed a priori
Now there's just a mountain of debugging to do, I'll hopefully come back later today to continue working on it

Thursday 11/10/16

Whelp everything is awful in the world
In terms of honors stuff, though, debugging goes smoothly this morning
It works! Now I'm super super overfitting
I'm instead going to validate on the entire validation set to avoid issues like this

Friday 11/11/16

Current open things to improve:
	output polyphonic music
	modularize multi-expert model and remove some of the magic numbers that made it work
	implement visualizer
	implement test
	write a script to test different architectures, including just regression, each subcombination, the multiple voice spacing models
	pickle weights for easy loading/saving


I did some preliminary experiments(just tried stuff out) and found that spacing + contour is the magic, it reduces error so fast, neither the simple generative nor the rhythm model seem to help much

I'm also looking for more data. This model fits very well, and I'd like to see what it does with a harder problem than bach chorales
I found this big collection of midi: https://www.reddit.com/r/datasets/comments/3akhxy/the_largest_midi_collection_on_the_internet/
it seems too heterogeneous on its own, but I may be able to get something out of it

http://musedata.stanford.edu/ is another possible good source

Monday 11/14/16
The national anthems dataset from the midi collection is a little harder to parse, but I think it might work. Benefit: Voices are split into separate channels based on instrument, though sometimes they overlap. The goal with this dataset is definitely to sound realistic, rather than serve as a score

My plan of action, then, is to get more nice-looking bach midi
or another composer, that would be fine

Challenges here:
	Quantization - can I round midi timestamps to nice numbers?
	More than 4 voices - my neural net architecture should be fine for, say, 18 voices, it just means I need to handle voices dynamically rather than statically

Tuesday 11/15/16

I'm going to write a testing framework that comes up with a visualization of each "expert" and their relative weights, and also graphs loss vs. minibatch number
Then script it to run for a bunch of different subsets of experts and minibatch sizes

I've come to the conclusion that visualization of actual neural network architectures are hard to do, and handwriting graphviz models might just work better

Thursday 11/17/16

On my own time I figured out the website things, I just need to write some content now, I'll deal with that tonight and show Kuperman tomorrow

Now I'm working on the visualization stuff
It works! Wonderful. I have pretty pictures.

Thursday 11/24/16

I'm on a plane home for thanksgiving working on my laptop. When I try to run things over SSH, I get a weird "Illegal action, core dumped" error. I'm not sure if this is a result of trying to run things over SSH (it shouldn't be, right?) or a bug in my code (I haven't tested much yet since finishing PoE).

In order to make things happen on my computer, I changed a bunch of file paths to be relative, not dependent on OCCS directory structure (I should have done this from the start, but whatever)

I also changed the way validation works - instead of randomly choosing four bars from somewhere in each validation piece, I choose a fixed-length prefix of each piece equal to the length of the shortest piece (numpy/theano really dislikes sparse arrays with the way I'm minibatching)

Saturday 11/26

Coming back to campus, what if instead of writing special visualization code for each expert, I just instantiate product of experts with one expert? That seems way better in hindsight.
How I'm getting visualization:

import train
dataset, min_num, max_num, timestep_length = train.load_dataset("../Data/train.p", "../Data/validate.p")
from Models.product_of_experts import MultiExpert
model = MultiExpert(['SimpleGenerative'], 4, 3, min_num, max_num, timestep_length, transparent=True)
train.train(model, 'JustSimpleGenerative', dataset, min_num, max_num, timestep_length, visualize=True)

and such

In order to generate full pieces, I want to change training to train on 2 voice and 3 voice subsets as well as the full 4 voice chorales. To do that, I propose randomly choosing for each sample from the training set, how many voices to take from it, then validate on the validation set with 2, 3 and 4 voices.

I also need a method for saving and loading weights, and a separate program that loads weights for a trained neural net, generates a voice (say, only using the contour expert, since it doesn't require any other data), then iteratively builds up the piece from there. This, I think, is the most interesting generative example because it doesn't depend on any of Bach's music directly.
This program should also have options to generate a voice given one or more of bach's voices for a piece

Monday 1/2/17

Happy New Year! Hopefully 2017 will be better than 2016 (though I'm not hopeful). I've been busy (but not taking notes because I suck) the past few days. Two important things: 
- I found a nasty off-by-one error in the way that I fixed the contour model that confirms that it has learned an identity - so it predicts notes with almost perfect certainty because it was being fed the current timestep as input. After fixing that, the contour outputs look significantly more like what I expected (a gaussian distribution over contours close to zero), but the product model still gets nearly perfect prediction accuracy.
- Someone published a paper doing exactly my project! https://www.csl.sony.fr/downloads/papers/2016/hadjeres-16a.pdf
They do a few things differently, and I'll probably write a blog post about it, basically they use a simple generative model trained forwards and backwards and a non-recurrent network predicting the a note given concurrent notes. Instead of taking the product of those experts, they train another neural network to predict the note given the otuputs of those networks, then use gibbs sampling to generate, rather than doing a single forward pass. They also train on all 12 transpositions of each piece, to avoid doing relative pitch stuff. I don't know which (if any) of those ideas I am interested in using, but this paper really changes the direction my final results should go.

Anyway, I'm going to keep writing retroactive blog posts and catch up with the research I've done so far.